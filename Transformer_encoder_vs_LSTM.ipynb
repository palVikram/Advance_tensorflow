{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/palVikram/Advance_tensorflow/blob/main/Transformer_encoder_vs_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liXGcB7lOmXu"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "vocab_size = 20000  # Only consider the top 20k words\n",
        "maxlen = 200  # Only consider the first 200 words of each movie review\n",
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5rwE2pzOmXv"
      },
      "source": [
        "## Implement a Transformer block as a layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rn_HPQqrOmXv"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self,**kwargs):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=2, key_dim=32)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(32, activation=\"relu\"), layers.Dense(32),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(0.1)\n",
        "        self.dropout2 = layers.Dropout(0.1)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({     \n",
        "            'att':self.att,\n",
        "            'ffn': self.ffn,\n",
        "            'layernorm1':self.layernorm1,\n",
        "            'layernorm2':self.layernorm2,\n",
        "            'dropout1':self.dropout1,\n",
        "            'dropout2':self.dropout2\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNsATKK2OmXw"
      },
      "source": [
        "## Implement embedding layer\n",
        "\n",
        "Two seperate embedding layers, one for tokens, one for token index (positions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LM5QR2OROmXw"
      },
      "outputs": [],
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=20000, output_dim=32)\n",
        "        self.pos_emb = layers.Embedding(input_dim=200, output_dim=32)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({     \n",
        "            'token_emb':self.token_emb,\n",
        "            'pos_emb': self.pos_emb\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def call(self, x):\n",
        "        ## X= VECTORIZATION(MY NAME IS VIKRAM AND MY WIFE NAME IS MANPREET KOUR)\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.range(start=0, limit=maxlen, delta=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HJ_1hb7k9fC",
        "outputId": "da1045c4-e5e4-4e39-9034-f3cd7ee17a01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(200,), dtype=int32, numpy=\n",
              "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
              "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
              "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
              "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
              "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
              "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
              "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
              "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
              "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
              "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
              "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
              "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
              "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
              "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
              "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
              "       195, 196, 197, 198, 199], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Zn6FXbUOmXx"
      },
      "source": [
        "## Download and prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtLRxHh5OmXy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aa2fb63-dce6-409c-8e56-615f3c47074b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000 Training sequences\n",
            "25000 Validation sequences\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
        "print(len(x_train), \"Training sequences\")\n",
        "print(len(x_val), \"Validation sequences\")\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSjiw2_SQoGg",
        "outputId": "d03a1133-119c-42c5-c1cc-063df49df4b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   5,   25,  100, ...,   19,  178,   32],\n",
              "       [   0,    0,    0, ...,   16,  145,   95],\n",
              "       [   0,    0,    0, ...,    7,  129,  113],\n",
              "       ...,\n",
              "       [   0,    0,    0, ...,    4, 3586,    2],\n",
              "       [   0,    0,    0, ...,   12,    9,   23],\n",
              "       [   0,    0,    0, ...,  204,  131,    9]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set(list(y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8AeJGBcOwa3",
        "outputId": "b1e3d12c-73dd-4fac-b2fa-4b141c739fe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1}"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThHSPr1GOmXz"
      },
      "source": [
        "## Create classifier model using transformer layer\n",
        "\n",
        "Transformer layer outputs one vector for each time step of our input sequence.\n",
        "Here, we take the mean across all time steps and\n",
        "use a feed forward network on top of it to classify text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADoULHIlOmXz"
      },
      "outputs": [],
      "source": [
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "x = TokenAndPositionEmbedding()(inputs)\n",
        "transformer_block = TransformerBlock()\n",
        "x = transformer_block(x)\n",
        "avg_pool = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "max_pool = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
        "x = tf.keras.layers.concatenate([avg_pool, max_pool])\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ2q5O_EOmX0"
      },
      "source": [
        "## Train and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DOr7frQOmX0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa189128-d28c-4f67-ac89-f080638a6e68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 104s 131ms/step - loss: 0.5182 - accuracy: 0.7080 - val_loss: 0.3163 - val_accuracy: 0.8691\n"
          ]
        }
      ],
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "history = model.fit(\n",
        "    x_train, y_train, batch_size=32, epochs=1, validation_data=(x_val, y_val)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = tf.keras.models.load_model('model.h5', custom_objects={'TokenAndPositionEmbedding': TokenAndPositionEmbedding, \\\n",
        "                                                                   'TransformerBlock': TransformerBlock})\n"
      ],
      "metadata": {
        "id": "kUgDnsLjO5Az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start = time.process_time()\n",
        "score = new_model.predict(np.array([x_val[0]]))\n",
        "print(time.process_time() - start)\n",
        "score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_BRKD5mH6ut",
        "outputId": "04e3bc82-99f2-4706-f57f-88fa0a9d5a07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.051662238000062644\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.927837  , 0.07216291]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=tf.keras.Sequential([\n",
        "          tf.keras.layers.Embedding(20000, 32, input_length=200),\n",
        "          tf.keras.layers.LSTM(units=256),\n",
        "          tf.keras.layers.Dense(2, activation='softmax')                       \n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "history = model.fit(\n",
        "    x_train, y_train, batch_size=32, epochs=1, validation_data=(x_val, y_val)\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4H4X-X0LAWs",
        "outputId": "3b55501d-0f24-4369-8832-c466691f19a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 560s 714ms/step - loss: 0.4618 - accuracy: 0.7840 - val_loss: 0.3470 - val_accuracy: 0.8535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start = time.process_time()\n",
        "score = model.predict(np.array([x_val[0]]))\n",
        "print(time.process_time() - start)\n",
        "score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtEWrchHUPmY",
        "outputId": "4a68e43a-9d5d-45ad-b01a-adab34d615fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.46727311299991925\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.8355441 , 0.16445588]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}