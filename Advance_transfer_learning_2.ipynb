{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "C3W1_Assignment.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/palVikram/Advance_tensorflow/blob/main/Advance_transfer_learning_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6Pnix2iKXR5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpiJj8ym0v0-"
      },
      "source": [
        "<a name=\"0-5\"></a>\n",
        "## 0.5 Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoilhmYe1b5t"
      },
      "source": [
        "import os, re, time, json\n",
        "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upfxqqK0vTMc"
      },
      "source": [
        "data_dir = \"/content/drive/My Drive/TF3 C3 W1 Data/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmoFKEd98MP3"
      },
      "source": [
        "<a name=\"1\"></a>\n",
        "## 1. Visualization Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWIHFPa0uOC_"
      },
      "source": [
        "def draw_bounding_box_on_image(image, ymin, xmin, ymax, xmax, color=(255, 0, 0), thickness=5):\n",
        "    \"\"\"\n",
        "    Adds a bounding box to an image.\n",
        "    Bounding box coordinates can be specified in either absolute (pixel) or\n",
        "    normalized coordinates by setting the use_normalized_coordinates argument.\n",
        "    \n",
        "    Args:\n",
        "      image: a PIL.Image object.\n",
        "      ymin: ymin of bounding box.\n",
        "      xmin: xmin of bounding box.\n",
        "      ymax: ymax of bounding box.\n",
        "      xmax: xmax of bounding box.\n",
        "      color: color to draw bounding box. Default is red.\n",
        "      thickness: line thickness. Default value is 4.\n",
        "    \"\"\"\n",
        "  \n",
        "    image_width = image.shape[1]\n",
        "    image_height = image.shape[0]\n",
        "    cv2.rectangle(image, (int(xmin), int(ymin)), (int(xmax), int(ymax)), color, thickness)\n",
        "\n",
        "\n",
        "def draw_bounding_boxes_on_image(image, boxes, color=[], thickness=5):\n",
        "    \"\"\"\n",
        "    Draws bounding boxes on image.\n",
        "    \n",
        "    Args:\n",
        "      image: a PIL.Image object.\n",
        "      boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n",
        "             The coordinates are in normalized format between [0, 1].\n",
        "      color: color to draw bounding box. Default is red.\n",
        "      thickness: line thickness. Default value is 4.\n",
        "                           \n",
        "    Raises:\n",
        "      ValueError: if boxes is not a [N, 4] array\n",
        "    \"\"\"\n",
        "    \n",
        "    boxes_shape = boxes.shape\n",
        "    if not boxes_shape:\n",
        "        return\n",
        "    if len(boxes_shape) != 2 or boxes_shape[1] != 4:\n",
        "        raise ValueError('Input must be of size [N, 4]')\n",
        "    for i in range(boxes_shape[0]):\n",
        "        draw_bounding_box_on_image(image, boxes[i, 1], boxes[i, 0], boxes[i, 3],\n",
        "                                 boxes[i, 2], color[i], thickness)\n",
        "\n",
        "\n",
        "def draw_bounding_boxes_on_image_array(image, boxes, color=[], thickness=5):\n",
        "    \"\"\"\n",
        "    Draws bounding boxes on image (numpy array).\n",
        "    \n",
        "    Args:\n",
        "      image: a numpy array object.\n",
        "      boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n",
        "             The coordinates are in normalized format between [0, 1].\n",
        "      color: color to draw bounding box. Default is red.\n",
        "      thickness: line thickness. Default value is 4.\n",
        "      display_str_list_list: a list of strings for each bounding box.\n",
        "    \n",
        "    Raises:\n",
        "      ValueError: if boxes is not a [N, 4] array\n",
        "    \"\"\"\n",
        "\n",
        "    draw_bounding_boxes_on_image(image, boxes, color, thickness)\n",
        "  \n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwJ4rZ1d_7ql"
      },
      "source": [
        "# Matplotlib config\n",
        "plt.rc('image', cmap='gray')\n",
        "plt.rc('grid', linewidth=0)\n",
        "plt.rc('xtick', top=False, bottom=False, labelsize='large')\n",
        "plt.rc('ytick', left=False, right=False, labelsize='large')\n",
        "plt.rc('axes', facecolor='F8F8F8', titlesize=\"large\", edgecolor='white')\n",
        "plt.rc('text', color='a8151a')\n",
        "plt.rc('figure', facecolor='F0F0F0')# Matplotlib fonts\n",
        "MATPLOTLIB_FONT_DIR = os.path.join(os.path.dirname(plt.__file__), \"mpl-data/fonts/ttf\")\n",
        "\n",
        "\n",
        "# utility to display a row of digits with their predictions\n",
        "def display_digits_with_boxes(images, pred_bboxes, bboxes, iou, title, bboxes_normalized=False):\n",
        "\n",
        "    n = len(images)\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 4))\n",
        "    plt.title(title)\n",
        "    plt.yticks([])\n",
        "    plt.xticks([])\n",
        "  \n",
        "    for i in range(n):\n",
        "      ax = fig.add_subplot(1, 10, i+1)\n",
        "      bboxes_to_plot = []\n",
        "      if (len(pred_bboxes) > i):\n",
        "        bbox = pred_bboxes[i]\n",
        "        bbox = [bbox[0] * images[i].shape[1], bbox[1] * images[i].shape[0], bbox[2] * images[i].shape[1], bbox[3] * images[i].shape[0]]\n",
        "        bboxes_to_plot.append(bbox)\n",
        "    \n",
        "      if (len(bboxes) > i):\n",
        "        bbox = bboxes[i]\n",
        "        if bboxes_normalized == True:\n",
        "          bbox = [bbox[0] * images[i].shape[1],bbox[1] * images[i].shape[0], bbox[2] * images[i].shape[1], bbox[3] * images[i].shape[0] ]\n",
        "        bboxes_to_plot.append(bbox)\n",
        "\n",
        "      img_to_draw = draw_bounding_boxes_on_image_array(image=images[i], boxes=np.asarray(bboxes_to_plot), color=[(255,0,0), (0, 255, 0)])\n",
        "      plt.xticks([])\n",
        "      plt.yticks([])\n",
        "    \n",
        "      plt.imshow(img_to_draw)\n",
        "\n",
        "      if len(iou) > i :\n",
        "        color = \"black\"\n",
        "        if (iou[i][0] < iou_threshold):\n",
        "          color = \"red\"\n",
        "        ax.text(0.2, -0.3, \"iou: %s\" %(iou[i][0]), color=color, transform=ax.transAxes)\n",
        "        \n",
        "        \n",
        "# utility to display training and validation curves\n",
        "def plot_metrics(metric_name, title, ylim=5):\n",
        "    plt.title(title)\n",
        "    plt.ylim(0,ylim)\n",
        "    plt.plot(history.history[metric_name],color='blue',label=metric_name)\n",
        "    plt.plot(history.history['val_' + metric_name],color='green',label='val_' + metric_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dg66jHMvw_f8"
      },
      "source": [
        "#### read_image_tfds\n",
        "- Resizes `image` to (224, 224)\n",
        "- Normalizes `image`\n",
        "- Translates and normalizes bounding boxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEEyTpmNxS0A"
      },
      "source": [
        "def read_image_tfds(image, bbox):\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    shape = tf.shape(image)\n",
        "\n",
        "    factor_x = tf.cast(shape[1], tf.float32)\n",
        "    factor_y = tf.cast(shape[0], tf.float32)\n",
        "\n",
        "    image = tf.image.resize(image, (224, 224,))\n",
        "\n",
        "    image = image/127.5\n",
        "    image -= 1\n",
        "\n",
        "    bbox_list = [bbox[0] / factor_x , \n",
        "                 bbox[1] / factor_y, \n",
        "                 bbox[2] / factor_x , \n",
        "                 bbox[3] / factor_y]\n",
        "    \n",
        "    return image, bbox_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxqvA3wkyH7p"
      },
      "source": [
        "#### read_image_with_shape\n",
        "This is very similar to `read_image_tfds` except it also keeps a copy of the original image (before pre-processing) and returns this as well.\n",
        "- Makes a copy of the original image.\n",
        "- Resizes `image` to (224, 224)\n",
        "- Normalizes `image`\n",
        "- Translates and normalizes bounding boxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f10wa31DyeQ4"
      },
      "source": [
        "def read_image_with_shape(image, bbox):\n",
        "    original_image = image\n",
        "    \n",
        "    image, bbox_list = read_image_tfds(image, bbox)\n",
        "    \n",
        "    return original_image, image, bbox_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNEpxvyLykzo"
      },
      "source": [
        "#### read_image_tfds_with_original_bbox\n",
        "\n",
        "- This function reads `image` from `data`\n",
        "- It also denormalizes the bounding boxes (it undoes the bounding box normalization that is performed by the previous two helper functions.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsQo9vvhyoKb"
      },
      "source": [
        "def read_image_tfds_with_original_bbox(data):\n",
        "    image = data[\"image\"]\n",
        "    bbox = data[\"bbox\"]\n",
        "\n",
        "    shape = tf.shape(image)\n",
        "    factor_x = tf.cast(shape[1], tf.float32) \n",
        "    factor_y = tf.cast(shape[0], tf.float32)\n",
        "\n",
        "    bbox_list = [bbox[1] * factor_x , \n",
        "                 bbox[0] * factor_y, \n",
        "                 bbox[3] * factor_x, \n",
        "                 bbox[2] * factor_y]\n",
        "    return image, bbox_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ElJ9VX0yui9"
      },
      "source": [
        "#### dataset_to_numpy_util\n",
        "This function converts a `dataset` into numpy arrays of images and boxes.\n",
        "- This will be used when visualizing the images and their bounding boxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CF-luxkJyzIA"
      },
      "source": [
        "def dataset_to_numpy_util(dataset, batch_size=0, N=0):\n",
        "\n",
        "    # eager execution: loop through datasets normally\n",
        "    take_dataset = dataset.shuffle(1024)\n",
        "\n",
        "    if batch_size > 0:\n",
        "        take_dataset = take_dataset.batch(batch_size)\n",
        "  \n",
        "    if N > 0:\n",
        "        take_dataset = take_dataset.take(N)\n",
        "  \n",
        "    if tf.executing_eagerly():\n",
        "        ds_images, ds_bboxes = [], []\n",
        "        for images, bboxes in take_dataset:\n",
        "            ds_images.append(images.numpy())\n",
        "            ds_bboxes.append(bboxes.numpy())\n",
        "        \n",
        "    return (np.array(ds_images), np.array(ds_bboxes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZSf8zvBy2RX"
      },
      "source": [
        "#### dataset_to_numpy_with_original_bboxes_util\n",
        "\n",
        "- This function converts a `dataset` into numpy arrays of \n",
        "  - original images\n",
        "  - resized and normalized images\n",
        "  - bounding boxes\n",
        "- This will be used for plotting the original images with true and predicted bounding boxes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZE8dgyPC1_6m"
      },
      "source": [
        "def dataset_to_numpy_with_original_bboxes_util(dataset, batch_size=0, N=0):\n",
        "\n",
        "    normalized_dataset = dataset.map(read_image_with_shape)\n",
        "    if batch_size > 0:\n",
        "        normalized_dataset = normalized_dataset.batch(batch_size)\n",
        "  \n",
        "    if N > 0:\n",
        "        normalized_dataset = normalized_dataset.take(N)\n",
        "\n",
        "    if tf.executing_eagerly():\n",
        "        ds_original_images, ds_images, ds_bboxes = [], [], []\n",
        "        \n",
        "    for original_images, images, bboxes in normalized_dataset:\n",
        "        ds_images.append(images.numpy())\n",
        "        ds_bboxes.append(bboxes.numpy())\n",
        "        ds_original_images.append(original_images.numpy())\n",
        "\n",
        "    return np.array(ds_original_images), np.array(ds_images), np.array(ds_bboxes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4gB0hprzMw4"
      },
      "source": [
        "Now you'll take a random sample of images from the training and validation sets and visualize them by plotting the corresponding bounding boxes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUPENeUHKXR6"
      },
      "source": [
        "Visualize the **training** images and their bounding box labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW_AyCNIKXR6"
      },
      "source": [
        "def get_visualization_training_dataset():      \n",
        "    dataset, info = tfds.load(\"caltech_birds2010\", split=\"train\", with_info=True, data_dir=data_dir, download=False)\n",
        "    print(info)\n",
        "    visualization_training_dataset = dataset.map(read_image_tfds_with_original_bbox, \n",
        "                                                 num_parallel_calls=16)\n",
        "    return visualization_training_dataset\n",
        "\n",
        "\n",
        "visualization_training_dataset = get_visualization_training_dataset()\n",
        "\n",
        "\n",
        "(visualization_training_images, visualization_training_bboxes) = dataset_to_numpy_util(visualization_training_dataset, N=10)\n",
        "display_digits_with_boxes(np.array(visualization_training_images), np.array([]), np.array(visualization_training_bboxes), np.array([]), \"training images and their bboxes\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qCuoUtYKXR6"
      },
      "source": [
        "Visualize the **validation** images and their bounding boxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLGiEyK_KXR6"
      },
      "source": [
        "def get_visualization_validation_dataset():\n",
        "    dataset = tfds.load(\"caltech_birds2010\", split=\"test\", data_dir=data_dir, download=False)\n",
        "    visualization_validation_dataset = dataset.map(read_image_tfds_with_original_bbox, num_parallel_calls=16)\n",
        "    return visualization_validation_dataset\n",
        "\n",
        "\n",
        "visualization_validation_dataset = get_visualization_validation_dataset()\n",
        "\n",
        "(visualization_validation_images, visualization_validation_bboxes) = dataset_to_numpy_util(visualization_validation_dataset, N=10)\n",
        "display_digits_with_boxes(np.array(visualization_validation_images), np.array([]), np.array(visualization_validation_bboxes), np.array([]), \"validation images and their bboxes\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5shayI_tzdq0"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "def get_training_dataset(dataset):\n",
        "    dataset = dataset.map(read_image_tfds, num_parallel_calls=16)\n",
        "    dataset = dataset.shuffle(512, reshuffle_each_iteration=True)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(-1) \n",
        "    return dataset\n",
        "\n",
        "def get_validation_dataset(dataset):\n",
        "    dataset = dataset.map(read_image_tfds, num_parallel_calls=16)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.repeat()\n",
        "    return dataset\n",
        "\n",
        "training_dataset = get_training_dataset(visualization_training_dataset)\n",
        "validation_dataset = get_validation_dataset(visualization_validation_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DFecRhe0Pqc"
      },
      "source": [
        "def feature_extractor(inputs):\n",
        "    ### YOUR CODE HERE ###\n",
        "    \n",
        "    # Create a mobilenet version 2 model object\n",
        "    mobilenet_model = tf.keras.applications.mobilenet_v2.MobileNetV2(\n",
        "    input_shape=(224,224,3), include_top=False, weights='imagenet')\n",
        "    \n",
        "    # pass the inputs into this mobile object to get a feature extractor for these inputs\n",
        "    feature_extractor = mobilenet_model(inputs)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "        \n",
        "    # return the feature_extractor\n",
        "    return feature_extractor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0njchQxB0b4Q"
      },
      "source": [
        "def dense_layers(features):\n",
        "    ### YOUR CODE HERE ###\n",
        "\n",
        "    # global average pooling 2d layer\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(features)     \n",
        "    \n",
        "    # flatten layer\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    \n",
        "    # 1024 Dense layer, with relu\n",
        "    x = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
        "    \n",
        "    # 512 Dense layer, with relu\n",
        "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdsD0-Jl07zW"
      },
      "source": [
        "def bounding_box_regression(x):\n",
        "    ### YOUR CODE HERE ###\n",
        "    \n",
        "    # Dense layer named `bounding_box`\n",
        "    bounding_box_regression_output = tf.keras.layers.Dense(units=4, name=\"bounding_box\")(x)\n",
        "\n",
        "    ### END CODE HERE###\n",
        "\n",
        "    return bounding_box_regression_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wn9O9c7I1XRJ"
      },
      "source": [
        "def final_model(inputs):\n",
        "    ### YOUR CODE HERE ###\n",
        "\n",
        "    # features\n",
        "    feature_cnn =feature_extractor(inputs)\n",
        "\n",
        "    # dense layers\n",
        "    last_dense_layer = dense_layers(feature_cnn) \n",
        "\n",
        "    # bounding box\n",
        "    bounding_box_output = bounding_box_regression(last_dense_layer)\n",
        "    \n",
        "    # define the TensorFlow Keras model using the inputs and outputs to your model\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=bounding_box_output)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C67ZmsTe1n9m"
      },
      "source": [
        "def define_and_compile_model():\n",
        "  \n",
        "    ### YOUR CODE HERE ###\n",
        "\n",
        "    # define the input layer\n",
        "    inputs = tf.keras.layers.Input(shape=(224,224,3))\n",
        "    \n",
        "    # create the model\n",
        "    model = final_model(inputs)\n",
        "    \n",
        "    # compile your model\n",
        "    SGD=tf.keras.optimizers.SGD(momentum=0.9)\n",
        "    model.compile(optimizer=SGD,\n",
        "                  loss='mse',\n",
        "                  metrics=['mse'])\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPtBf83B1zZ3"
      },
      "source": [
        "Run the cell below to define your model and print the model summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56y8UNFQIVwj"
      },
      "source": [
        "# define your model\n",
        "model = define_and_compile_model()\n",
        "# print model layers\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtVVYVlvKXR7"
      },
      "source": [
        "<a name='4'></a>\n",
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoIY6xQ_KXR7"
      },
      "source": [
        "# You'll train 50 epochs\n",
        "EPOCHS = 50\n",
        "\n",
        "### START CODE HERE ###\n",
        "## 'test': 3033,\n",
        "## 'train': 3000,\n",
        "\n",
        "# Choose a batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Get the length of the training set\n",
        "length_of_training_dataset = 3000\n",
        "\n",
        "# Get the length of the validation set\n",
        "length_of_validation_dataset = 3033\n",
        "\n",
        "# Get the steps per epoch (may be a few lines of code)\n",
        "steps_per_epoch = length_of_training_dataset//BATCH_SIZE\n",
        "if length_of_training_dataset % BATCH_SIZE > 0:\n",
        "    steps_per_epoch += 1\n",
        "\n",
        "# get the validation steps (per epoch) (may be a few lines of code)\n",
        "validation_steps = length_of_validation_dataset//BATCH_SIZE\n",
        "if length_of_validation_dataset % BATCH_SIZE > 0:\n",
        "    validation_steps += 1\n",
        "    \n",
        "### END CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ####\n",
        "\n",
        "# Fit the model, setting the parameters noted in the instructions above.\n",
        "history = model.fit(training_dataset,\n",
        "                    steps_per_epoch=steps_per_epoch, validation_data=validation_dataset, validation_steps=validation_steps, epochs=EPOCHS)\n",
        "\n",
        "### END CODE HERE ###"
      ],
      "metadata": {
        "id": "-0Ws_LpLxmHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWbkUql5sAok"
      },
      "source": [
        "loss = model.evaluate(validation_dataset, steps=validation_steps)\n",
        "print(\"Loss: \", loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Cvv-GgvE3V4"
      },
      "source": [
        "# Please save your model\n",
        "model.save(\"birds.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW2AAdkRsOMP"
      },
      "source": [
        "# And download it using this shortcut or from the \"Files\" panel to the left\n",
        "from google.colab import files\n",
        "\n",
        "files.download(\"birds.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz-b8TxU6EDj"
      },
      "source": [
        "plot_metrics(\"loss\", \"Bounding Box Loss\", ylim=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFqJxt3_VrCm"
      },
      "source": [
        "def intersection_over_union(pred_box, true_box):\n",
        "\n",
        "    xmin_pred, ymin_pred, xmax_pred, ymax_pred =  np.split(pred_box, 4, axis = 1)\n",
        "    xmin_true, ymin_true, xmax_true, ymax_true = np.split(true_box, 4, axis = 1)\n",
        "\n",
        "    #Calculate coordinates of overlap area between boxes\n",
        "    xmin_overlap = np.maximum(xmin_pred, xmin_true)\n",
        "    xmax_overlap = np.minimum(xmax_pred, xmax_true)\n",
        "    ymin_overlap = np.maximum(ymin_pred, ymin_true)\n",
        "    ymax_overlap = np.minimum(ymax_pred, ymax_true)\n",
        "\n",
        "    #Calculates area of true and predicted boxes\n",
        "    pred_box_area = (xmax_pred - xmin_pred) * (ymax_pred - ymin_pred)\n",
        "    true_box_area = (xmax_true - xmin_true) * (ymax_true - ymin_true)\n",
        "\n",
        "    #Calculates overlap area and union area.\n",
        "    overlap_area = np.maximum((xmax_overlap - xmin_overlap),0)  * np.maximum((ymax_overlap - ymin_overlap), 0)\n",
        "    union_area = (pred_box_area + true_box_area) - overlap_area\n",
        "\n",
        "    # Defines a smoothing factor to prevent division by 0\n",
        "    smoothing_factor = 1e-10\n",
        "\n",
        "    #Updates iou score\n",
        "    iou = (overlap_area + smoothing_factor) / (union_area + smoothing_factor)\n",
        "\n",
        "    return iou\n",
        "\n",
        "#Makes predictions\n",
        "original_images, normalized_images, normalized_bboxes = dataset_to_numpy_with_original_bboxes_util(visualization_validation_dataset, N=500)\n",
        "predicted_bboxes = model.predict(normalized_images, batch_size=32)\n",
        "\n",
        "\n",
        "#Calculates IOU and reports true positives and false positives based on IOU threshold\n",
        "iou = intersection_over_union(predicted_bboxes, normalized_bboxes)\n",
        "iou_threshold = 0.5\n",
        "\n",
        "print(\"Number of predictions where iou > threshold(%s): %s\" % (iou_threshold, (iou >= iou_threshold).sum()))\n",
        "print(\"Number of predictions where iou < threshold(%s): %s\" % (iou_threshold, (iou < iou_threshold).sum()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR9Bb4uCwTyw"
      },
      "source": [
        "n = 10\n",
        "indexes = np.random.choice(len(predicted_bboxes), size=n)\n",
        "\n",
        "iou_to_draw = iou[indexes]\n",
        "norm_to_draw = original_images[indexes]\n",
        "display_digits_with_boxes(original_images[indexes], predicted_bboxes[indexes], normalized_bboxes[indexes], iou[indexes], \"True and Predicted values\", bboxes_normalized=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qlT1g6jn9CM3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}