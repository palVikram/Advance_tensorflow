{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1396d6d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers) (3.12.4)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Downloading huggingface_hub-0.20.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->transformers) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.20.1-py3-none-any.whl (330 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.20.1 safetensors-0.4.1 tokenizers-0.15.0 transformers-4.36.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00a71507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e62fc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 40  # Maximum length of input sentence to the model.\n",
    "batch_size = 128\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5e4a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4d285f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_length = 40  # Maximum length of input sentence to the model.\n",
    "batch_size = 64\n",
    "epochs = 2\n",
    "\n",
    "class BertSemanticDataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"Generates batches of data.\n",
    "\n",
    "    Args:\n",
    "        sentences: Array of input sentences.\n",
    "        labels: Array of labels.\n",
    "        batch_size: Integer batch size.\n",
    "        shuffle: boolean, whether to shuffle the data.\n",
    "        include_targets: boolean, whether to include the labels.\n",
    "\n",
    "    Returns:\n",
    "        Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n",
    "        (or just `[input_ids, attention_mask, `token_type_ids]`\n",
    "         if `include_targets=False`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sentences,\n",
    "        labels,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        include_targets=True,\n",
    "    ):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.include_targets = include_targets\n",
    "        # Load our BERT Tokenizer to encode the text.\n",
    "        # We will use base-base-uncased pretrained model.\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "        self.indexes = np.arange(len(self.sentences))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Denotes the number of batches per epoch.\n",
    "        return len(self.sentences) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieves the batch of index.\n",
    "        indexes = self.indexes[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "        sentences = [self.sentences[i] for i in indexes]\n",
    "\n",
    "        # Single sentence input.\n",
    "        encoded = self.tokenizer(\n",
    "            sentences,\n",
    "            max_length=max_length,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            pad_to_max_length=True,\n",
    "            return_tensors=\"tf\",\n",
    "        )\n",
    "\n",
    "        # Rest of the code remains unchanged.\n",
    "        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
    "        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
    "        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
    "\n",
    "        if self.include_targets:\n",
    "            labels = np.array(self.labels[indexes], dtype=\"int32\")\n",
    "            \n",
    "            # Ensure that labels have the expected shape\n",
    "            # Modify this part based on your actual label structure\n",
    "            target = {\n",
    "                'output_layer_1': labels[:, 0],\n",
    "                'output_layer_2': labels[:, 1],\n",
    "                'output_layer_3': labels[:, 2],\n",
    "                'output_layer_4': labels[:, 3],\n",
    "                'output_layer_5': labels[:, 4],\n",
    "                'output_layer_6': labels[:, 5],\n",
    "            }\n",
    "\n",
    "            return [input_ids, attention_masks, token_type_ids], target\n",
    "        else:\n",
    "            return [input_ids, attention_masks, token_type_ids]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle indexes after each epoch if shuffle is set to True.\n",
    "        if self.shuffle:\n",
    "            np.random.RandomState(42).shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52a815ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:            15G        1.5G         10G        676K        3.5G         13G\r\n",
      "Swap:            0B          0B          0B\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3586b6e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56af8a8db0246dc8ba795eb310fe2b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c55eb258e7c41f0840b6bbda4938161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)      [(None, 40)]                 0         []                            \n",
      "                                                                                                  \n",
      " attention_masks (InputLaye  [(None, 40)]                 0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " token_type_ids (InputLayer  [(None, 40)]                 0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bert (TFBertMainLayer)      TFBaseModelOutputWithPooli   1778534   ['input_ids[0][0]',           \n",
      "                             ngAndCrossAttentions(last_   40         'attention_masks[0][0]',     \n",
      "                             hidden_state=(None, 40, 76              'token_type_ids[0][0]']      \n",
      "                             8),                                                                  \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  (None, 40, 128)              426496    ['bert[0][0]']                \n",
      " al)                                                                                              \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 128)                  0         ['bidirectional[0][0]']       \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d (Glob  (None, 128)                  0         ['bidirectional[0][0]']       \n",
      " alMaxPooling1D)                                                                                  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 256)                  0         ['global_average_pooling1d[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'global_max_pooling1d[0][0]']\n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)        (None, 256)                  0         ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " output_layer_1 (Dense)      (None, 4)                    1028      ['dropout_37[0][0]']          \n",
      "                                                                                                  \n",
      " output_layer_2 (Dense)      (None, 4)                    1028      ['dropout_37[0][0]']          \n",
      "                                                                                                  \n",
      " output_layer_3 (Dense)      (None, 4)                    1028      ['dropout_37[0][0]']          \n",
      "                                                                                                  \n",
      " output_layer_4 (Dense)      (None, 4)                    1028      ['dropout_37[0][0]']          \n",
      "                                                                                                  \n",
      " output_layer_5 (Dense)      (None, 4)                    1028      ['dropout_37[0][0]']          \n",
      "                                                                                                  \n",
      " output_layer_6 (Dense)      (None, 4)                    1028      ['dropout_37[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 178286104 (680.11 MB)\n",
      "Trainable params: 178286104 (680.11 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model under a distribution strategy scope.\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    # Encoded token ids from xlm roberta base tokenizer.\n",
    "    input_ids = tf.keras.layers.Input(\n",
    "        shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n",
    "    )\n",
    "    # Attention masks indicates to the model which tokens should be attended to.\n",
    "    attention_masks = tf.keras.layers.Input(\n",
    "        shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n",
    "    )\n",
    "    # Token type ids are binary masks identifying different sequences in the model.\n",
    "    token_type_ids = tf.keras.layers.Input(\n",
    "        shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n",
    "    )\n",
    "    \n",
    "    # Loading pretrained xlm roberta base model.\n",
    "    bert_model = TFBertModel.from_pretrained(\"bert-base-multilingual-cased\") \n",
    "    # Freeze the xlm roberta base model model to reuse the pretrained features without modifying them.\n",
    "    bert_model.trainable = True\n",
    "\n",
    "    bert_output = bert_model.bert(\n",
    "        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n",
    "    )\n",
    "    sequence_output = bert_output.last_hidden_state\n",
    "    pooled_output = bert_output.pooler_output\n",
    "     \n",
    "    # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n",
    "    bi_lstm = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True)\n",
    "    )(sequence_output)\n",
    "    # Applying hybrid pooling approach to bi_lstm sequence output.\n",
    "    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n",
    "    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n",
    "    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n",
    "\n",
    "    dropout = tf.keras.layers.Dropout(0.3)(concat)\n",
    "\n",
    "    # Add Dense layers for each classification task with 4 units (assuming 4 classes) and softmax activation.\n",
    "    output_layer_1 = tf.keras.layers.Dense(4, activation='softmax', name='output_layer_1')(dropout)\n",
    "    output_layer_2 = tf.keras.layers.Dense(4, activation='softmax', name='output_layer_2')(dropout)\n",
    "    output_layer_3 = tf.keras.layers.Dense(4, activation='softmax', name='output_layer_3')(dropout)\n",
    "    output_layer_4 = tf.keras.layers.Dense(4, activation='softmax', name='output_layer_4')(dropout)\n",
    "    output_layer_5 = tf.keras.layers.Dense(4, activation='softmax', name='output_layer_5')(dropout)\n",
    "    output_layer_6 = tf.keras.layers.Dense(4, activation='softmax', name='output_layer_6')(dropout)\n",
    "\n",
    "    # Create the model\n",
    "    model = tf.keras.Model(\n",
    "        inputs=[input_ids, attention_masks, token_type_ids],\n",
    "        outputs=[output_layer_1, output_layer_2,output_layer_3,output_layer_4,output_layer_5, output_layer_6]\n",
    "    )\n",
    "    \n",
    "    losses = {\n",
    "        'output_layer_1': 'sparse_categorical_crossentropy',\n",
    "        'output_layer_2': 'sparse_categorical_crossentropy',\n",
    "        'output_layer_3': 'sparse_categorical_crossentropy',\n",
    "        'output_layer_4': 'sparse_categorical_crossentropy',\n",
    "        'output_layer_5': 'sparse_categorical_crossentropy',\n",
    "        'output_layer_6': 'sparse_categorical_crossentropy',\n",
    "    }\n",
    "\n",
    "    # Compile the model with an appropriate loss function and optimizer\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "        loss=losses,  \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e508b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"content_moderation_multilingual_direct_fine_tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29b993e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>rating</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>강간</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>개새끼</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>개자식</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>개좆</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>개차반</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536690</th>\n",
       "      <td>1551883</td>\n",
       "      <td>whore</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536691</th>\n",
       "      <td>1551884</td>\n",
       "      <td>whores</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536692</th>\n",
       "      <td>1551885</td>\n",
       "      <td>willy-whacker</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536693</th>\n",
       "      <td>1551886</td>\n",
       "      <td>wise ass</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536694</th>\n",
       "      <td>1551887</td>\n",
       "      <td>wnker</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1536695 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0.2   comment_text  toxic  obscene  identity_attack  insult  \\\n",
       "0                   0             강간      3        3                0       2   \n",
       "1                   1            개새끼      3        3                0       2   \n",
       "2                   2            개자식      3        3                0       2   \n",
       "3                   3             개좆      3        3                0       2   \n",
       "4                   4            개차반      3        3                0       2   \n",
       "...               ...            ...    ...      ...              ...     ...   \n",
       "1536690       1551883          whore      3        3                0       0   \n",
       "1536691       1551884         whores      3        3                0       0   \n",
       "1536692       1551885  willy-whacker      3        3                0       0   \n",
       "1536693       1551886       wise ass      1        1                0       0   \n",
       "1536694       1551887          wnker      1        1                0       0   \n",
       "\n",
       "         threat  sexual_explicit  Unnamed: 0.1  Unnamed: 0  id rating  length  \n",
       "0             0                3           NaN         NaN NaN    NaN     NaN  \n",
       "1             0                3           NaN         NaN NaN    NaN     NaN  \n",
       "2             0                3           NaN         NaN NaN    NaN     NaN  \n",
       "3             0                3           NaN         NaN NaN    NaN     NaN  \n",
       "4             0                3           NaN         NaN NaN    NaN     NaN  \n",
       "...         ...              ...           ...         ...  ..    ...     ...  \n",
       "1536690       0                3           NaN         NaN NaN    NaN     NaN  \n",
       "1536691       0                3           NaN         NaN NaN    NaN     NaN  \n",
       "1536692       0                3           NaN         NaN NaN    NaN     NaN  \n",
       "1536693       0                1           NaN         NaN NaN    NaN     NaN  \n",
       "1536694       0                1           NaN         NaN NaN    NaN     NaN  \n",
       "\n",
       "[1536695 rows x 13 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df=pd.read_csv('combined_enhaced_multiligual_dataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b77ae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "672a2b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "183c511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the DataFrame into train, validation, and test sets\n",
    "train_ratio = 0.8  # 70% for training\n",
    "valid_ratio = 0.1  # 15% for validation\n",
    "test_ratio = 0.1   # 15% for testing\n",
    "\n",
    "train_df, test_valid_df = train_test_split(shuffled_df, test_size=(valid_ratio + test_ratio), random_state=42)\n",
    "valid_df, test_df = train_test_split(test_valid_df, test_size=test_ratio/(valid_ratio + test_ratio), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28d864ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>rating</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>515977</th>\n",
       "      <td>523189</td>\n",
       "      <td>ไม ่ น ่ าแปลกใจเลยที ่ ผู ้ ชมของเขาส ่ วนใหญ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>520523.0</td>\n",
       "      <td>3452812.0</td>\n",
       "      <td>5097197.0</td>\n",
       "      <td>approved</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133286</th>\n",
       "      <td>1146653</td>\n",
       "      <td>\"Copying my comments and using them is nothing...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1143987.0</td>\n",
       "      <td>274497.0</td>\n",
       "      <td>5325394.0</td>\n",
       "      <td>approved</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416438</th>\n",
       "      <td>1431393</td>\n",
       "      <td>BB é Burrito Boy! Lol! Vá lá, homem. \"\" Ei, va...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1428727.0</td>\n",
       "      <td>2182963.0</td>\n",
       "      <td>4983591.0</td>\n",
       "      <td>approved</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523727</th>\n",
       "      <td>1538918</td>\n",
       "      <td>Psihopatija i genetika, bacanje kockica u tko ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1536252.0</td>\n",
       "      <td>2103152.0</td>\n",
       "      <td>6082030.0</td>\n",
       "      <td>approved</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072074</th>\n",
       "      <td>1084967</td>\n",
       "      <td>Tell that guy, Don Young, to put Native Land i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1082301.0</td>\n",
       "      <td>23820.0</td>\n",
       "      <td>346666.0</td>\n",
       "      <td>approved</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531559</th>\n",
       "      <td>538952</td>\n",
       "      <td>Bedankt voor dat stukje informatie!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>536286.0</td>\n",
       "      <td>1127534.0</td>\n",
       "      <td>329169.0</td>\n",
       "      <td>approved</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400595</th>\n",
       "      <td>406322</td>\n",
       "      <td>Правильно... изучение всего. Genius вывод.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>403656.0</td>\n",
       "      <td>1323212.0</td>\n",
       "      <td>792024.0</td>\n",
       "      <td>approved</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61491</th>\n",
       "      <td>62476</td>\n",
       "      <td>그렇죠. 이 괴물은 이미 D I E가 되도록 합시다!</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>59810.0</td>\n",
       "      <td>627379.0</td>\n",
       "      <td>5551166.0</td>\n",
       "      <td>approved</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500436</th>\n",
       "      <td>507478</td>\n",
       "      <td>http://i.onionstatic.com/avclub/5807/95/animat...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>504812.0</td>\n",
       "      <td>1367049.0</td>\n",
       "      <td>5360184.0</td>\n",
       "      <td>approved</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562222</th>\n",
       "      <td>569968</td>\n",
       "      <td>आप किसी दूसरे को मूर्ख समझते समय अपनी उद्धरण च...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>567302.0</td>\n",
       "      <td>3844408.0</td>\n",
       "      <td>308568.0</td>\n",
       "      <td>approved</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>307339 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0.2                                       comment_text  \\\n",
       "515977         523189  ไม ่ น ่ าแปลกใจเลยที ่ ผู ้ ชมของเขาส ่ วนใหญ...   \n",
       "1133286       1146653  \"Copying my comments and using them is nothing...   \n",
       "1416438       1431393  BB é Burrito Boy! Lol! Vá lá, homem. \"\" Ei, va...   \n",
       "1523727       1538918  Psihopatija i genetika, bacanje kockica u tko ...   \n",
       "1072074       1084967  Tell that guy, Don Young, to put Native Land i...   \n",
       "...               ...                                                ...   \n",
       "531559         538952                Bedankt voor dat stukje informatie!   \n",
       "400595         406322         Правильно... изучение всего. Genius вывод.   \n",
       "61491           62476                      그렇죠. 이 괴물은 이미 D I E가 되도록 합시다!   \n",
       "500436         507478  http://i.onionstatic.com/avclub/5807/95/animat...   \n",
       "562222         569968  आप किसी दूसरे को मूर्ख समझते समय अपनी उद्धरण च...   \n",
       "\n",
       "         toxic  obscene  identity_attack  insult  threat  sexual_explicit  \\\n",
       "515977       0        0                0       0       0                0   \n",
       "1133286      0        0                0       0       0                0   \n",
       "1416438      1        0                0       0       0                1   \n",
       "1523727      0        0                0       0       0                0   \n",
       "1072074      0        0                0       0       0                0   \n",
       "...        ...      ...              ...     ...     ...              ...   \n",
       "531559       0        0                0       0       0                0   \n",
       "400595       0        0                0       0       0                0   \n",
       "61491        2        0                0       2       2                0   \n",
       "500436       0        0                0       0       0                0   \n",
       "562222       3        1                0       3       0                0   \n",
       "\n",
       "         Unnamed: 0.1  Unnamed: 0         id    rating  length  \n",
       "515977       520523.0   3452812.0  5097197.0  approved    14.0  \n",
       "1133286     1143987.0    274497.0  5325394.0  approved    14.0  \n",
       "1416438     1428727.0   2182963.0  4983591.0  approved    15.0  \n",
       "1523727     1536252.0   2103152.0  6082030.0  approved    11.0  \n",
       "1072074     1082301.0     23820.0   346666.0  approved    19.0  \n",
       "...               ...         ...        ...       ...     ...  \n",
       "531559       536286.0   1127534.0   329169.0  approved     6.0  \n",
       "400595       403656.0   1323212.0   792024.0  approved     6.0  \n",
       "61491         59810.0    627379.0  5551166.0  approved    10.0  \n",
       "500436       504812.0   1367049.0  5360184.0  approved     1.0  \n",
       "562222       567302.0   3844408.0   308568.0  approved    17.0  \n",
       "\n",
       "[307339 rows x 13 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "063f39ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=train_df[[\"toxic\",\"obscene\",\"identity_attack\",\"insult\",\"threat\",\"sexual_explicit\"]]\n",
    "train_df=train_df[['comment_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20eb4750",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val=valid_df[[\"toxic\",\"obscene\",\"identity_attack\",\"insult\",\"threat\",\"sexual_explicit\"]]\n",
    "valid_df=valid_df[['comment_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c7b6077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 1, 0, 0],\n",
       "       ...,\n",
       "       [2, 0, 0, 2, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [3, 0, 1, 1, 3, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c6a2c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1206043</th>\n",
       "      <td>Clayton Ruby quá già để được bổ nhiệm vào toà ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530061</th>\n",
       "      <td>১২ বছর বয়সী মেয়ে? ওখানে কট ্ টরটা খুব কষ ্ ট</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121022</th>\n",
       "      <td>I agree he is a carpet bagger only using Alaskans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486133</th>\n",
       "      <td>So, it would appear, is attacking Coptic Chris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458026</th>\n",
       "      <td>Guns dont kill people, Never-Trumpers kill peo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259178</th>\n",
       "      <td>Putting Chief in front of Harrison is like put...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414414</th>\n",
       "      <td>Ainda gostam de bater nas mulheres, não é?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131932</th>\n",
       "      <td>Miten pääsette näin naurettavaan johtopäätökseen?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671155</th>\n",
       "      <td>Samo to dodaj na svoj popis \"\" bojkota \"\", i n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121958</th>\n",
       "      <td>Låt mig säga att varenda västerländsk land som...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1229355 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              comment_text\n",
       "1206043  Clayton Ruby quá già để được bổ nhiệm vào toà ...\n",
       "530061      ১২ বছর বয়সী মেয়ে? ওখানে কট ্ টরটা খুব কষ ্ ট\n",
       "1121022  I agree he is a carpet bagger only using Alaskans\n",
       "1486133  So, it would appear, is attacking Coptic Chris...\n",
       "1458026  Guns dont kill people, Never-Trumpers kill peo...\n",
       "...                                                    ...\n",
       "259178   Putting Chief in front of Harrison is like put...\n",
       "1414414         Ainda gostam de bater nas mulheres, não é?\n",
       "131932   Miten pääsette näin naurettavaan johtopäätökseen?\n",
       "671155   Samo to dodaj na svoj popis \"\" bojkota \"\", i n...\n",
       "121958   Låt mig säga att varenda västerländsk land som...\n",
       "\n",
       "[1229355 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0757b59d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1206043    Clayton Ruby quá già để được bổ nhiệm vào toà ...\n",
       "530061        ১২ বছর বয়সী মেয়ে? ওখানে কট ্ টরটা খুব কষ ্ ট\n",
       "1121022    I agree he is a carpet bagger only using Alaskans\n",
       "1486133    So, it would appear, is attacking Coptic Chris...\n",
       "1458026    Guns dont kill people, Never-Trumpers kill peo...\n",
       "                                 ...                        \n",
       "259178     Putting Chief in front of Harrison is like put...\n",
       "1414414           Ainda gostam de bater nas mulheres, não é?\n",
       "131932     Miten pääsette näin naurettavaan johtopäätökseen?\n",
       "671155     Samo to dodaj na svoj popis \"\" bojkota \"\", i n...\n",
       "121958     Låt mig säga att varenda västerländsk land som...\n",
       "Name: comment_text, Length: 1229355, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff4eaa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=test_df[[\"toxic\",\"obscene\",\"identity_attack\",\"insult\",\"threat\",\"sexual_explicit\"]]\n",
    "test_df=test_df[['comment_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b4f2e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef952362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf332589bf04428a720eb2aa6324c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e19eddc41cf43f5a6cf6d17fc88b797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b116ecd95d864d6e8d3ebcbef1e62e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 6s 6s/step\n",
      "[array([[9.2707801e-01, 7.0355073e-02, 1.8183644e-03, 7.4845157e-04]],\n",
      "      dtype=float32), array([[9.7906303e-01, 2.0244712e-02, 4.8491333e-04, 2.0733858e-04]],\n",
      "      dtype=float32), array([[9.8480761e-01, 1.4902597e-02, 2.1750614e-04, 7.2212883e-05]],\n",
      "      dtype=float32), array([[9.5497638e-01, 4.3522153e-02, 9.4604929e-04, 5.5536482e-04]],\n",
      "      dtype=float32), array([[9.8920751e-01, 1.0605867e-02, 1.4993074e-04, 3.6730999e-05]],\n",
      "      dtype=float32), array([[9.8955101e-01, 1.0162842e-02, 2.1426505e-04, 7.1917173e-05]],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "sentence_pairs = np.array([\"My name is Vikram Pal CEO of AI Intelli Inc.\"])\n",
    "test_data = BertSemanticDataGenerator(\n",
    "    sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n",
    ")\n",
    "\n",
    "proba = model.predict(test_data[0])\n",
    "\n",
    "print(proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6dd9a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>sexual_explicit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>386491</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388016</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454119</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523035</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115128</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431347</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811294</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755672</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21619</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707753</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153669 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         toxic  obscene  identity_attack  insult  threat  sexual_explicit\n",
       "386491       1        0                0       0       0                0\n",
       "1388016      1        0                0       1       0                0\n",
       "1454119      1        0                0       1       0                0\n",
       "523035       1        1                0       0       1                1\n",
       "1115128      0        0                0       0       0                0\n",
       "...        ...      ...              ...     ...     ...              ...\n",
       "1431347      0        0                0       0       0                0\n",
       "811294       0        0                1       0       0                0\n",
       "755672       2        0                0       2       0                0\n",
       "21619        1        0                0       0       0                0\n",
       "707753       3        0                1       1       3                0\n",
       "\n",
       "[153669 rows x 6 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c25fe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_data = BertSemanticDataGenerator(\n",
    "    train_df['comment_text'].tolist(),\n",
    "    y_train.values,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "valid_data = BertSemanticDataGenerator(\n",
    "    valid_df['comment_text'].tolist(),\n",
    "    y_val.values,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9804ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26eb28dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1190590, 1038285, 1028391, ...,  131932,  671155,  121958])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26a7a111",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "19208/19208 [==============================] - ETA: 0s - loss: 2.4029 - output_layer_1_loss: 0.8121 - output_layer_2_loss: 0.2431 - output_layer_3_loss: 0.2758 - output_layer_4_loss: 0.7247 - output_layer_5_loss: 0.2182 - output_layer_6_loss: 0.1290 - output_layer_1_accuracy: 0.6471 - output_layer_2_accuracy: 0.9304 - output_layer_3_accuracy: 0.9029 - output_layer_4_accuracy: 0.6857 - output_layer_5_accuracy: 0.9307 - output_layer_6_accuracy: 0.9688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19208/19208 [==============================] - 9402s 487ms/step - loss: 2.4029 - output_layer_1_loss: 0.8121 - output_layer_2_loss: 0.2431 - output_layer_3_loss: 0.2758 - output_layer_4_loss: 0.7247 - output_layer_5_loss: 0.2182 - output_layer_6_loss: 0.1290 - output_layer_1_accuracy: 0.6471 - output_layer_2_accuracy: 0.9304 - output_layer_3_accuracy: 0.9029 - output_layer_4_accuracy: 0.6857 - output_layer_5_accuracy: 0.9307 - output_layer_6_accuracy: 0.9688 - val_loss: 2.2602 - val_output_layer_1_loss: 0.7703 - val_output_layer_2_loss: 0.2330 - val_output_layer_3_loss: 0.2553 - val_output_layer_4_loss: 0.6820 - val_output_layer_5_loss: 0.1996 - val_output_layer_6_loss: 0.1200 - val_output_layer_1_accuracy: 0.6683 - val_output_layer_2_accuracy: 0.9323 - val_output_layer_3_accuracy: 0.9101 - val_output_layer_4_accuracy: 0.7077 - val_output_layer_5_accuracy: 0.9353 - val_output_layer_6_accuracy: 0.9698\n",
      "Epoch 2/2\n",
      "  660/19208 [>.............................] - ETA: 2:24:25 - loss: 2.2222 - output_layer_1_loss: 0.7483 - output_layer_2_loss: 0.2268 - output_layer_3_loss: 0.2606 - output_layer_4_loss: 0.6670 - output_layer_5_loss: 0.1999 - output_layer_6_loss: 0.1196 - output_layer_1_accuracy: 0.6791 - output_layer_2_accuracy: 0.9345 - output_layer_3_accuracy: 0.9088 - output_layer_4_accuracy: 0.7149 - output_layer_5_accuracy: 0.9343 - output_layer_6_accuracy: 0.9701"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=valid_data,\n",
    "    epochs=epochs,\n",
    "    use_multiprocessing=True,\n",
    "    workers=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ab74e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "8356\n",
    "\n",
    "7468"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d3db5a",
   "metadata": {},
   "source": [
    "# Unfreeze the bert_model.\n",
    "bert_model.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c184c586",
   "metadata": {},
   "source": [
    "# Recompile the model to make the change effective.\n",
    "# Change loss function to mean squared error for regression\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[\"mae\"],  # You can use other metrics like \"mse\" or custom metrics as needed\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e480cc3",
   "metadata": {},
   "source": [
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=valid_data,\n",
    "    epochs=epochs,\n",
    "    use_multiprocessing=True,\n",
    "    workers=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1305249b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: content_moderation_multilingual_direct_fine_tuningv02/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: content_moderation_multilingual_direct_fine_tuningv02/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"content_moderation_multilingual_direct_fine_tuningv02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1fa6279a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2401/2401 [==============================] - 378s 157ms/step - loss: 2.2643 - output_layer_1_loss: 0.7715 - output_layer_2_loss: 0.2285 - output_layer_3_loss: 0.2566 - output_layer_4_loss: 0.6839 - output_layer_5_loss: 0.2053 - output_layer_6_loss: 0.1185 - output_layer_1_accuracy: 0.6658 - output_layer_2_accuracy: 0.9332 - output_layer_3_accuracy: 0.9098 - output_layer_4_accuracy: 0.7080 - output_layer_5_accuracy: 0.9330 - output_layer_6_accuracy: 0.9704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.2643024921417236,\n",
       " 0.7714725732803345,\n",
       " 0.22849474847316742,\n",
       " 0.2566157877445221,\n",
       " 0.6838600635528564,\n",
       " 0.20531369745731354,\n",
       " 0.1185464933514595,\n",
       " 0.6658098101615906,\n",
       " 0.9332244396209717,\n",
       " 0.909783661365509,\n",
       " 0.7080187797546387,\n",
       " 0.9330357313156128,\n",
       " 0.9703834056854248]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = BertSemanticDataGenerator(\n",
    "    test_df['comment_text'].tolist(),\n",
    "    y_test.values,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "model.evaluate(test_data, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bc351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=[train_data[0][0][0][0],train_data[0][0][1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829afd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_valid_df[test_valid_df['obscene']>0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b7f5cb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 37ms/step\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "sentence_pairs = [\"sei una sorella stronza\"]\n",
    "test_data = BertSemanticDataGenerator(\n",
    "        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n",
    "    )\n",
    "\n",
    "proba = model.predict(test_data[0])\n",
    "\n",
    "### \"toxic\",\"obscene\",\"identity_attack\",\"insult\",\"threat\",\"sexual_explicit\"\n",
    "for prob in proba:\n",
    "    idx = np.argmax(prob[0])\n",
    "    prob = f\"{prob[0][idx]: .2f}%\"\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1bb190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d8b531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf0df90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc9e240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b34094c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb08b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa314a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a9846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35abf3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b089e199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4993fb4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10e64d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8ce15b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeed1a40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a252ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data cleaning and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74caaf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"multilingual_content_moderation_text_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65a7e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "num=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14180c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toxic_total=df[(df[\"toxic\"]>num) | (df[\"severe_toxicity\"]>num) | (df[\"obscene\"]>num) | (df[\"identity_attack\"]>num) | (df[\"insult\"]>num) | (df[\"threat\"]>num) | (df[\"sexual_explicit\"]>num)]\n",
    "df_toxic_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5108542",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal=df[(df[\"toxic\"]==0) & (df[\"severe_toxicity\"]==0) & (df[\"obscene\"]==0) & (df[\"identity_attack\"]==0) & (df[\"insult\"]==0) & (df[\"threat\"]==0) & (df[\"sexual_explicit\"]==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98242c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"toxic\", \"severe_toxicity\", \"obscene\", \"identity_attack\", \"insult\", \"threat\", \"sexual_explicit\"]] = df[[\"toxic\", \"severe_toxicity\", \"obscene\", \"identity_attack\", \"insult\", \"threat\", \"sexual_explicit\"]].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da767678",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction = 1/6\n",
    "df_sample = df_normal.sample(frac=fraction, random_state=42)  # Set a random seed for reproducibility\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b0be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_total = pd.concat([df_toxic_total, df_sample], axis=0, ignore_index=True)\n",
    "df_combined_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a7d10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### <.1 ---> 0  normal\n",
    "### 0.1-0.45 ---> 1 low risk \n",
    "### 0.45 --> 0.70 --> 2 medium risk \n",
    "### >.70 --> 3 --> High risk \n",
    "\n",
    "def risk_calculation(x):\n",
    "    if x<=0.1:\n",
    "        return 0\n",
    "    elif x>0.1 and x<=0.45: \n",
    "        return 1\n",
    "    elif x>0.45 and x<=0.70:\n",
    "        return 2 \n",
    "    else:\n",
    "        return 3 \n",
    "    \n",
    "    \n",
    "# Specify the columns to apply the risk calculation function\n",
    "columns_to_apply = ['toxic', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat', 'sexual_explicit']\n",
    "\n",
    "# Apply the risk calculation function to the specified columns\n",
    "df_combined_total[columns_to_apply] = df_combined_total[columns_to_apply].applymap(risk_calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f9d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_total.drop([\"severe_toxicity\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef97f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e7336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the DataFrame\n",
    "df_combined_total = df_combined_total.sample(frac=1, random_state=42)  # Set a random seed for reproducibility\n",
    "\n",
    "# Reset the index after shuffling\n",
    "df_combined_total = df_combined_total.reset_index(drop=True)\n",
    "\n",
    "df_combined_total.to_csv(\"classificaiton_dataframe_content_moderation_v02.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18160cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1698837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip \"List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words-master (1).zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5da021",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set the directory path where your text files are located\n",
    "directory_path = \"./List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words-master/\"\n",
    "\n",
    "# Create an empty DataFrame to store the data\n",
    "df = pd.DataFrame(columns=['text', 'language_code'])\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "    \n",
    "    # Check if the file is a text file (you can add more conditions based on your naming conventions)\n",
    "    if os.path.isfile(file_path) and not filename.startswith('.'):\n",
    "        # Read the content of the text file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text_lines = file.readlines()\n",
    "\n",
    "        # Extract language code from the filename\n",
    "        language_code = filename\n",
    "\n",
    "        # Append each line as a separate row to the DataFrame\n",
    "        for line in text_lines:\n",
    "            df = df.append({'text': line.strip(), 'language_code': language_code}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bd753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment_text']=df['text']\n",
    "\n",
    "df.drop(['text', 'language_code'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bdcf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profanity=pd.read_csv(\"profanity_en.csv\")\n",
    "df_profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade52ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profanity[\"severity_description\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9dd113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "df_profanity['severity_description'] = df_profanity['severity_description'].replace({'Strong': 3, 'Severe': 2, 'Mild': 1})\n",
    "df_profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab993fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profanity=df_profanity[(df_profanity[\"category_1\"]==\"sexual anatomy / sexual acts\") | (df_profanity[\"category_1\"]==\"sexual orientation / gender\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bded6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9844a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profanity[\"sexual_explicit\"]=df_profanity[\"severity_description\"]\n",
    "df_profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3cedde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profanity['comment_text']=df_profanity['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c37c454",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profanity=df_profanity[[\"comment_text\",\"sexual_explicit\"]]\n",
    "df_profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f33b891",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profanity['toxic']=df_profanity['sexual_explicit']\n",
    "df_profanity['obscene']=df_profanity['sexual_explicit']\n",
    "\n",
    "df_profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2375be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_profanity['identity_attack']=0\n",
    "df_profanity['insult']=0\n",
    "df_profanity['threat']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ec2baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb76d696",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['toxic']=3\n",
    "df[ 'obscene']=3\n",
    "df['identity_attack']=0\n",
    "df['insult']=2\n",
    "df['threat']=0\n",
    "df['sexual_explicit']=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d86f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8ec6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c=pd.read_csv(\"classificaiton_dataframe_content_moderation_v02.csv\")\n",
    "\n",
    "df_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589885b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined=pd.concat([df, df_c, df_profanity], axis=0, ignore_index=True)\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79036f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['obscene'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c138a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = df_combined.drop_duplicates(subset=['comment_text'], keep='last')\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990de5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.to_csv(\"combined_enhaced_multiligual_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa2eef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73349c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c956966",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip Suicide_Detection.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4734b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_self_harm=pd.read_csv(\"Suicide_Detection.csv\")\n",
    "df_self_harm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad7475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_self_harm['length']=df_self_harm[\"text\"].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd85867",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_self_harm['length'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77118a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_self_harm[df_self_harm['class']==\"suicide\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5761fe5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
