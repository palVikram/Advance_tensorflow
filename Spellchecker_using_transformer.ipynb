{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode\n",
        "!pip install keras_nlp"
      ],
      "metadata": {
        "id": "0fR30hiu451D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d910c93-0177-4816-df58-cd7553042932"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 KB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras_nlp\n",
            "  Downloading keras_nlp-0.4.0-py3-none-any.whl (337 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.5/337.5 KB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from keras_nlp) (1.4.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (from keras_nlp) (2.11.0)\n",
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from keras_nlp) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from keras_nlp) (23.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (23.1.21)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (2.11.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (2.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (4.5.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (15.0.6.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (3.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (1.51.1)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (2.11.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (0.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (57.4.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (3.19.6)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (2.11.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (0.30.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (1.15.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-text->keras_nlp) (0.12.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow->keras_nlp) (0.38.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (2.25.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (2.16.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (3.4.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (6.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (2022.12.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (3.13.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (3.2.2)\n",
            "Installing collected packages: tensorflow-text, keras_nlp\n",
            "Successfully installed keras_nlp-0.4.0 tensorflow-text-2.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(1234)\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import re\n",
        "import os\n",
        "import tensorflow as tf \n",
        "import unidecode\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "error_rate = 0.8\n",
        "hidden_size = 512\n",
        "nb_epochs = 100\n",
        "train_batch_size = 512\n",
        "val_batch_size = 256\n",
        "sample_mode = 'argmax'\n",
        "\n",
        "reverse = True"
      ],
      "metadata": {
        "id": "9CBy2jeP46jG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOS = '\\t' # start of sequence.\n",
        "EOS = '*' # end of sequence.\n",
        "CHARS = list('abcdefghijklmnopqrstuvwxyz')\n",
        "REMOVE_CHARS = '[#$%\"\\+@<=>!&,-.?:;()*\\[\\]^_`{|}~/\\d\\t\\n\\r\\x0b\\x0c]'"
      ],
      "metadata": {
        "id": "pN5_KsrC4-rD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_speling_erors(token, error_rate):\n",
        "    \"\"\"Simulate some artificial spelling mistakes.\"\"\"\n",
        "    assert(0.0 <= error_rate < 1.0)\n",
        "    if len(token) < 3:\n",
        "        return token\n",
        "    rand = np.random.rand()\n",
        "    # Here are 4 different ways spelling mistakes can occur,\n",
        "    # each of which has equal chance.\n",
        "    prob = error_rate / 4.0\n",
        "    if rand < prob:\n",
        "        # Replace a character with a random character.\n",
        "        random_char_index = np.random.randint(len(token))\n",
        "        token = token[:random_char_index] + np.random.choice(CHARS) \\\n",
        "                + token[random_char_index + 1:]\n",
        "    elif prob < rand < prob * 2:\n",
        "        # Delete a character.\n",
        "        random_char_index = np.random.randint(len(token))\n",
        "        token = token[:random_char_index] + token[random_char_index + 1:]\n",
        "    elif prob * 2 < rand < prob * 3:\n",
        "        # Add a random character.\n",
        "        random_char_index = np.random.randint(len(token))\n",
        "        token = token[:random_char_index] + np.random.choice(CHARS) \\\n",
        "                + token[random_char_index:]\n",
        "    elif prob * 3 < rand < prob * 4:\n",
        "        # Transpose 2 characters.\n",
        "        random_char_index = np.random.randint(len(token) - 1)\n",
        "        token = token[:random_char_index]  + token[random_char_index + 1] \\\n",
        "                + token[random_char_index] + token[random_char_index + 2:]\n",
        "    else:\n",
        "        # Replace a character with a random character.\n",
        "        random_char_index = np.random.randint(len(token))\n",
        "        token = token[:random_char_index] + np.random.choice(CHARS) \\\n",
        "                + token[random_char_index + 1:]\n",
        "                \n",
        "    return token\n"
      ],
      "metadata": {
        "id": "jp-clmob4_6M"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform(tokens, maxlen, error_rate=0.3, shuffle=True):\n",
        "    \"\"\"Transform tokens into model inputs and targets.\n",
        "    All inputs and targets are padded to maxlen with EOS character.\n",
        "    \"\"\"\n",
        "    if shuffle:\n",
        "        print('Shuffling data.')\n",
        "        np.random.shuffle(tokens)\n",
        "    encoder_tokens = []\n",
        "    decoder_tokens = []\n",
        "    target_tokens = []\n",
        "    for token in tokens:\n",
        "        encoder = add_speling_erors(token, error_rate=error_rate)\n",
        "        encoder += EOS * (maxlen - len(encoder)) # Padded to maxlen.\n",
        "        encoder_tokens.append(encoder)\n",
        "    \n",
        "        decoder = SOS + token\n",
        "        decoder += EOS * (maxlen - len(decoder))\n",
        "        decoder_tokens.append(decoder)\n",
        "    \n",
        "        target = decoder[1:]\n",
        "        target += EOS * (maxlen - len(target))\n",
        "        target_tokens.append(target)\n",
        "        \n",
        "        assert(len(encoder) == len(decoder) == len(target))\n",
        "\n",
        "    for token in tokens:\n",
        "        encoder = add_speling_erors(token, error_rate=error_rate)\n",
        "        encoder += EOS * (maxlen - len(encoder)) # Padded to maxlen.\n",
        "        encoder_tokens.append(encoder)\n",
        "    \n",
        "        decoder = SOS + token\n",
        "        decoder += EOS * (maxlen - len(decoder))\n",
        "        decoder_tokens.append(decoder)\n",
        "    \n",
        "        target = decoder[1:]\n",
        "        target += EOS * (maxlen - len(target))\n",
        "        target_tokens.append(target)\n",
        "        \n",
        "        assert(len(encoder) == len(decoder) == len(target))\n",
        "\n",
        "    for token in tokens:\n",
        "        encoder = token\n",
        "        encoder += EOS * (maxlen - len(encoder)) # Padded to maxlen.\n",
        "        encoder_tokens.append(encoder)\n",
        "        \n",
        "        decoder = SOS + token\n",
        "        decoder += EOS * (maxlen - len(decoder))\n",
        "        decoder_tokens.append(decoder)\n",
        "    \n",
        "        target = decoder[1:]\n",
        "        target += EOS * (maxlen - len(target))\n",
        "        target_tokens.append(target)\n",
        "        \n",
        "        assert(len(encoder) == len(decoder) == len(target))\n",
        "    return encoder_tokens, decoder_tokens, target_tokens"
      ],
      "metadata": {
        "id": "jVmboAAn5ECa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('30k.txt', 'r') as file:\n",
        "    vocab = file.readlines()\n",
        "\n",
        "vocab=[word.strip() for word in vocab if len(word)>1]"
      ],
      "metadata": {
        "id": "em46p3gd5GOE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('10k.txt', 'r') as file:\n",
        "    val_tokens = file.readlines()\n",
        "\n",
        "val_tokens=[word.strip() for word in val_tokens if len(word)>1]"
      ],
      "metadata": {
        "id": "8rU0e7gt5J4s"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 512\n",
        "EPOCHS = 1  # This should be at least 10 for convergence\n",
        "MAX_SEQUENCE_LENGTH = 40\n",
        "ENG_VOCAB_SIZE = 30000\n",
        "SPA_VOCAB_SIZE = 30000\n",
        "\n",
        "EMBED_DIM = 256\n",
        "INTERMEDIATE_DIM = 2048\n",
        "NUM_HEADS = 8"
      ],
      "metadata": {
        "id": "hXJQFt4L5Lqr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = list(filter(None, set(vocab)))\n",
        "\n",
        "# `maxlen` is the length of the longest word in the vocabulary\n",
        "# plus two SOS and EOS characters.\n",
        "maxlen = max([len(token) for token in vocab]) + 2\n",
        "train_encoder, train_decoder, train_target = transform(\n",
        "    vocab, maxlen, error_rate=error_rate, shuffle=False)\n",
        "print(train_encoder[:10])\n",
        "print(train_decoder[:10])\n",
        "print(train_target[:10])\n",
        "\n",
        "input_chars = set(' '.join(train_encoder))\n",
        "target_chars = set(' '.join(train_decoder))\n",
        "nb_input_chars = len(input_chars)\n",
        "nb_target_chars = len(target_chars)\n",
        "\n",
        "print('Size of training vocabulary =', len(vocab))\n",
        "print('Number of unique input characters:', nb_input_chars)\n",
        "print('Number of unique target characters:', nb_target_chars)\n",
        "print('Max sequence length in the training set:', maxlen)\n",
        "\n",
        "# Prepare validation data.\n",
        "val_tokens = list(filter(None, val_tokens))\n",
        "\n",
        "val_maxlen = max([len(token) for token in val_tokens]) + 2\n",
        "val_encoder, val_decoder, val_target = transform(\n",
        "    val_tokens, maxlen, error_rate=error_rate, shuffle=False)\n",
        "print(val_encoder[:10])\n",
        "print(val_decoder[:10])\n",
        "print(val_target[:10])\n",
        "print('Number of non-unique validation tokens =', len(val_tokens))\n",
        "print('Max sequence length in the validation set:', val_maxlen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVY4t1CR5QH6",
        "outputId": "6a019fbe-d34f-4997-deb4-fdff548539a6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['placemvnt******************', 'zwsj***********************', 'allergy********************', 'masn***********************', 'wps************************', 'esigner********************', 'jamount********************', 'guqrdians******************', 'gardicner******************', 'iniiatives*****************']\n",
            "['\\tplacement*****************', '\\twsj***********************', '\\tallergy*******************', '\\tmason*********************', '\\tips***********************', '\\tdesigner******************', '\\tamount********************', '\\tguardians*****************', '\\tgardiner******************', '\\tinitiatives***************']\n",
            "['placement******************', 'wsj************************', 'allergy********************', 'mason**********************', 'ips************************', 'designer*******************', 'amount*********************', 'guardians******************', 'gardiner*******************', 'initiatives****************']\n",
            "Size of training vocabulary = 30000\n",
            "Number of unique input characters: 28\n",
            "Number of unique target characters: 29\n",
            "Max sequence length in the training set: 27\n",
            "['he*************************', 'of*************************', 'nad************************', 'to*************************', 'a**************************', 'in*************************', 'for************************', 'is*************************', 'on*************************', 'thit***********************']\n",
            "['\\tthe***********************', '\\tof************************', '\\tand***********************', '\\tto************************', '\\ta*************************', '\\tin************************', '\\tfor***********************', '\\tis************************', '\\ton************************', '\\tthat**********************']\n",
            "['the************************', 'of*************************', 'and************************', 'to*************************', 'a**************************', 'in*************************', 'for************************', 'is*************************', 'on*************************', 'that***********************']\n",
            "Number of non-unique validation tokens = 10000\n",
            "Max sequence length in the validation set: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through each character in the string and encode it\n",
        "def encoding(string):\n",
        "  encoded_string = []\n",
        "  for character in string:\n",
        "      encoded_string.append(ord(character))\n",
        "  return np.array(encoded_string)\n",
        "\n",
        "print(encoding(\"hello\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ym2qpHC15TYK",
        "outputId": "3ee6b731-9340-4898-b60c-4b96f09d43f0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[104 101 108 108 111]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def batch(tokens, maxlen, ctable, batch_size=128, reverse=False):\n",
        "  data_batchs=[]\n",
        "  for token in tokens:\n",
        "    data_batch = encoding(token)\n",
        "    data_batchs.append(data_batch)\n",
        "  return data_batchs"
      ],
      "metadata": {
        "id": "cfF6A8lP5USi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encoder_batch = batch(train_encoder, maxlen,\n",
        "                            train_batch_size, reverse)  \n",
        "\n",
        "train_decoder_batch = batch(train_decoder, maxlen,\n",
        "                            train_batch_size)\n",
        "train_target_batch  = batch(train_target, maxlen,\n",
        "                            train_batch_size)   \n",
        " \n",
        "val_encoder_batch = batch(val_encoder, maxlen,\n",
        "                          val_batch_size, reverse)\n",
        "val_decoder_batch = batch(val_decoder, maxlen,\n",
        "                          val_batch_size)\n",
        "val_target_batch  = batch(val_target, maxlen,\n",
        "                          val_batch_size)"
      ],
      "metadata": {
        "id": "SoqHJmc25X6r"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_batch(train_encoder_batch, train_decoder_batch,train_target_batch):\n",
        "    return (\n",
        "        {\n",
        "            \"encoder_inputs\": train_encoder_batch,\n",
        "            \"decoder_inputs\": train_decoder_batch,\n",
        "        },\n",
        "        train_target_batch,\n",
        "    )\n",
        "\n",
        "\n",
        "def make_dataset(train_encoder_batch, train_decoder_batch,train_target_batch):\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((train_encoder_batch, train_decoder_batch,train_target_batch))\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.map(preprocess_batch, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_encoder_batch, train_decoder_batch,train_target_batch)\n",
        "val_ds = make_dataset(val_encoder_batch,val_decoder_batch,val_target_batch)"
      ],
      "metadata": {
        "id": "gET5rsp_5cci"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")\n",
        "    print(inputs[\"decoder_inputs\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRz73jYo5daC",
        "outputId": "9a548b3b-67c2-491d-bc5b-b21aa6c047a3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (512, 27)\n",
            "inputs[\"decoder_inputs\"].shape: (512, 27)\n",
            "targets.shape: (512, 27)\n",
            "tf.Tensor(\n",
            "[  9 103 114  97 112 101  42  42  42  42  42  42  42  42  42  42  42  42\n",
            "  42  42  42  42  42  42  42  42  42], shape=(27,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 1  # This should be at least 10 for convergence\n",
        "MAX_SEQUENCE_LENGTH = 30\n",
        "\n",
        "EMBED_DIM = 512\n",
        "INTERMEDIATE_DIM = 512\n",
        "NUM_HEADS = 16\n",
        "\n",
        "import keras \n",
        "import keras_nlp\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "\n",
        "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=50000,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    embedding_dim=EMBED_DIM,\n",
        "    mask_zero=True,\n",
        ")(encoder_inputs)\n",
        "x = keras.layers.Dropout(0.3)(x)\n",
        "encoder_outputs=x\n",
        "\n",
        "for _ in range(2):\n",
        "  encoder_outputs = keras_nlp.layers.TransformerEncoder(\n",
        "        intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
        "    )(inputs=encoder_outputs)\n",
        "  encoder_outputs = keras.layers.Dropout(0.3)(encoder_outputs)\n",
        "\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name=\"decoder_state_inputs\")\n",
        "\n",
        "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=50000,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    embedding_dim=EMBED_DIM,\n",
        "    mask_zero=True,\n",
        ")(decoder_inputs)\n",
        "\n",
        "for _ in range(2):\n",
        "  x = keras_nlp.layers.TransformerDecoder(\n",
        "        intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
        "    )(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\n",
        "  x = keras.layers.Dropout(0.3)(x)\n",
        "\n",
        "decoder_outputs = keras.layers.Dense(150, activation=\"softmax\")(x)\n",
        "decoder = keras.Model(\n",
        "    [\n",
        "        decoder_inputs,\n",
        "        encoded_seq_inputs,\n",
        "    ],\n",
        "    decoder_outputs,\n",
        ")\n",
        "\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs],\n",
        "    decoder_outputs,\n",
        "    name=\"transformer\",\n",
        ")\n",
        "\n",
        "\n",
        "transformer.compile(\n",
        "    \"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "QUxPicbN5iYy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate.\n",
        "no_epochs=500\n",
        "for epoch in range(no_epochs):\n",
        "    print('Main Epoch {:d}/{:d}'.format(epoch + 1, no_epochs))\n",
        "\n",
        "    train_encoder, train_decoder, train_target = transform(\n",
        "        vocab, maxlen, error_rate=error_rate, shuffle=True)\n",
        "    \n",
        "    train_encoder_batch = batch(train_encoder, maxlen,\n",
        "                            train_batch_size, reverse)  \n",
        "\n",
        "    train_decoder_batch = batch(train_decoder, maxlen,\n",
        "                                train_batch_size)\n",
        "    train_target_batch  = batch(train_target, maxlen,\n",
        "                                train_batch_size)   \n",
        "    \n",
        "    val_encoder_batch = batch(val_encoder, maxlen,\n",
        "                              val_batch_size, reverse)\n",
        "    val_decoder_batch = batch(val_decoder, maxlen,\n",
        "                              val_batch_size)\n",
        "    val_target_batch  = batch(val_target, maxlen,\n",
        "                              val_batch_size)\n",
        "    train_ds = make_dataset(train_encoder_batch, train_decoder_batch,train_target_batch)\n",
        "    val_ds = make_dataset(val_encoder_batch,val_decoder_batch,val_target_batch)\n",
        "\n",
        "    transformer.fit(train_ds, epochs=1, validation_data=val_ds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALr6Evpr5zyS",
        "outputId": "2f892180-1fd5-4963-ae33-d5c76fd0860b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Main Epoch 1/500\n",
            "Shuffling data.\n",
            "272/352 [======================>.......] - ETA: 31s - loss: 1.4163 - accuracy: 0.7416"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encoding_input(string):\n",
        "  encoded_string = []\n",
        "  length=len(string)\n",
        "\n",
        "  total_pending=27-length\n",
        "  string=list(string)+[EOS]*total_pending\n",
        "  for character in string:\n",
        "      encoded_string.append(ord(character))\n",
        "  # Convert the list to a tensor\n",
        "  encoded_string = tf.convert_to_tensor(encoded_string)\n",
        "\n",
        "  # Reshape the tensor to have shape (1, 27)\n",
        "  encoded_string = tf.reshape(encoded_string, [1, 27])\n",
        "  return encoded_string\n",
        "\n",
        "tokenized_input_sentence=encoding_input(\"gamng\")\n",
        "tokenized_input_sentence\n",
        "\n",
        "\n",
        "tokenized_input_sentence"
      ],
      "metadata": {
        "id": "O2PO0zyRywqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(word):\n",
        "  decoder_input_tokens=encoding_input(SOS)\n",
        "  tokenized_input_sentence=encoding_input(word)\n",
        "  result=''\n",
        "  for i in range(maxlen):\n",
        "    predictions=transformer([tokenized_input_sentence, decoder_input_tokens])\n",
        "    sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "    decoder_input_tokens = tf.tensor_scatter_nd_update(decoder_input_tokens, [[0, i+1]], [sampled_token_index])\n",
        "    \n",
        "    if sampled_token_index==42:\n",
        "      break\n",
        "    result=result+chr(sampled_token_index)\n",
        "  return result\n",
        "\n",
        "inference(\"informtion\")"
      ],
      "metadata": {
        "id": "wTgieHN2-vWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WEsczm0QDcMV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}